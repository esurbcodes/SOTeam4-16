#!/usr/bin/env python3

# SWE 45000, PIN FALL 2025
# TEAM 4
# PHASE 1 PROJECT

# DISCLAIMER: This file contains code either partially or entirely written by
# Artificial Intelligence.
"""
Executable CLI 'run' for Phase 1.

Usage:
  ./run install        -> installs dependencies from requirements.txt
  ./run test           -> runs test suite
  ./run URL_FILE       -> processes newline-delimited URLs and prints NDJSON
"""
from __future__ import annotations # Allows annotations (like return types) to be postponed and interpreted as strings

# ----------------------------
# Standard library imports
# ----------------------------
import argparse      # for parsing command line arguments
import importlib     # for dynamic module importing
import json          # for encoding/decoding JSON
import logging       # for logging info/errors
import os            # for environment variables & file operations
import pkgutil       # for discovering Python modules
import subprocess    # for running external processes
import sys           # for system-specific functions
import time
import shutil
import stat
import io
from contextlib import redirect_stdout, redirect_stderr


from concurrent.futures import ThreadPoolExecutor, as_completed  # for parallel tasks
from pathlib import Path       # for safer path operations
from typing import Any, Dict, List, Tuple, Callable  # type hints

os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")
os.environ.setdefault("TQDM_DISABLE", "1")  # extra belt-and-suspenders
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")


REPO_ROOT = Path(__file__).resolve().parent
SRC_DIR = REPO_ROOT / "src"

# Make imports like `import src...` work in this process
for p in (str(REPO_ROOT), str(SRC_DIR)):
    if p not in sys.path:
        sys.path.insert(0, p)

# Ensure child processes (pytest/coverage) inherit it too
_prev = os.environ.get("PYTHONPATH", "")
parts = [str(REPO_ROOT), str(SRC_DIR)]
if _prev:
    parts.append(_prev)
os.environ["PYTHONPATH"] = os.pathsep.join(parts)


# ----------------------------
# Logging setup (reads env)
# ----------------------------

LOG_FILE = os.environ.get("LOG_FILE")
try:
    LOG_LEVEL_ENV = int(os.environ.get("LOG_LEVEL", "0"))
except ValueError:
    LOG_LEVEL_ENV = 0

# 0 -> disable logging output, 1 -> INFO, 2 -> DEBUG
level_map = {0: 100, 1: logging.INFO, 2: logging.DEBUG}  # 100 > CRITICAL disables output

handlers = []
if LOG_FILE:
    try:
        handlers = [logging.FileHandler(LOG_FILE, mode="w", encoding="utf-8")]
    except Exception:
        # Invalid path: fall back to stderr instead of crashing
        handlers = [logging.StreamHandler(sys.stderr)]
else:
    handlers = [logging.StreamHandler(sys.stderr)]

logging.basicConfig(
    level=level_map.get(LOG_LEVEL_ENV, 100),
    handlers=handlers,
    format="%(asctime)s %(levelname)s %(message)s"
)
logger = logging.getLogger("phase1_cli")


def remove_readonly(func, path, excinfo):
    """Error handler for shutil.rmtree that removes read-only permissions."""
    os.chmod(path, stat.S_IWRITE)
    func(path)

# ----------------------------
# Install / Test handlers
# ----------------------------
def run_subprocess(cmd: List[str]) -> int: # cmd is a variable of type: List[string], and (-> int), means return type int
    """Run a subprocess command and return exit code."""
    try:
        result = subprocess.run(cmd, check=False) 
        return result.returncode
    except Exception as exc:  # safety net
        logger.error("Subprocess failed: %s", exc)
        return 1
        
# --- AUTOGRADER BOOTSTRAP HELPERS ---
def _have(mod: str) -> bool:
    try:
        import importlib.util
        return importlib.util.find_spec(mod) is not None
    except Exception:
        return False

def _pip_install(*args: str) -> int:
    # install into the exact interpreter the grader is using
    return run_subprocess([sys.executable, "-m", "pip", *args])

def ensure_test_deps() -> None:
    # runtime deps
    if Path("requirements.txt").exists():
        _pip_install("install", "--no-cache-dir", "-r", "requirements.txt")
    # dev/test deps
    if Path("requirements-dev.txt").exists():
        _pip_install("install", "--no-cache-dir", "-r", "requirements-dev.txt")
    # fallback if grader ignored requirements-dev.txt
    if not (_have("pytest") and _have("coverage")):
        _pip_install("install", "--no-cache-dir", "pytest", "pytest-cov", "pytest-mock", "coverage")



def handle_install() -> int:
    """Install dependencies from requirements.txt (and dev deps if present)."""
    rc = 0
    req = Path("requirements.txt")
    if req.exists():
        rc = run_subprocess([sys.executable, "-m", "pip", "install", "-r", str(req)])
        if rc != 0:
            logger.error("Dependency installation failed (exit %d)", rc)
            return rc

    dev = Path("requirements-dev.txt")
    if dev.exists():
        run_subprocess([sys.executable, "-m", "pip", "install", "-r", str(dev)])

    return rc

def _ensure_test_tools() -> None:
    """Best-effort install pytest/coverage/pytest-mock if missing (quiet, no crash)."""
    try:
        import importlib.util, subprocess, sys
        needed = []
        for mod in ("pytest", "coverage", "pytest_mock"):
            if importlib.util.find_spec(mod) is None:
                # package name differs for pytest-mock:
                needed.append("pytest-mock" if mod == "pytest_mock" else mod)
        if needed:
            subprocess.run(
                [sys.executable, "-m", "pip", "install", "--no-cache-dir", *needed],
                check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
            )
    except Exception:
        pass

def handle_test() -> int:
    """
    Run the test suite once under coverage and print exactly:
    'X/Y test cases passed. Z% line coverage achieved.'

    Returns:
      0 if pytest exit code is 0 (tests passed), otherwise 1.
    """

    import tempfile
    import xml.etree.ElementTree as ET
    import re
    
    _ensure_test_tools()

    # Create a temporary file for JUnit XML output (mkstemp avoids Windows locking problems)
    junit_fd, junit_path = tempfile.mkstemp(suffix=".xml")
    os.close(junit_fd)

    try:
        # Clear any previous coverage data to get a fresh measurement
        subprocess.run([sys.executable, "-m", "coverage", "erase"], capture_output=True, text=True)

        # Run tests once under coverage and produce JUnit XML for reliable counts
        proc = subprocess.run(
            [sys.executable, "-m", "coverage", "run", "-m", "pytest", "-q", f"--junitxml={junit_path}"],
            capture_output=True,
            text=True
        )
        pytest_rc = proc.returncode

        # Default totals
        total = 0
        passed = 0

        # Try to parse JUnit XML for robust counts (supports single testsuite or multiple)
        try:
            tree = ET.parse(junit_path)
            root = tree.getroot()

            # collect all <testsuite> nodes (root may be testsuite or testsuites)
            suites = []
            if root.tag == "testsuite":
                suites = [root]
            else:
                # findall with .// to capture nested testsuites too
                suites = root.findall(".//testsuite")
                # if still empty and root itself has attributes, try root as a suite
                if not suites and "tests" in root.attrib:
                    suites = [root]

            for s in suites:
                tests = int(s.attrib.get("tests", "0"))
                failures = int(s.attrib.get("failures", "0"))
                errors = int(s.attrib.get("errors", "0"))
                skipped = int(s.attrib.get("skipped", "0"))
                total += tests
                passed += max(0, tests - failures - errors - skipped)

        except Exception:
            # Fallback: try to infer from pytest stdout if JUnit parsing fails
            out = proc.stdout or ""
            # Try to capture "collected N items" or "N passed" patterns
            m_collected = re.search(r"collected\s+(\d+)", out)
            if m_collected:
                total = int(m_collected.group(1))
            m_passed = re.search(r"(\d+)\s+passed", out)
            if m_passed:
                passed = int(m_passed.group(1))
            # As last resort, if we have dots in stdout (rare with -q), count them
            if total == 0:
                dot_count = out.count(".")
                if dot_count:
                    passed = dot_count
                    total = dot_count

        # Compute coverage percentage by parsing coverage report output
        coverage_pct = "0"
        try:
            cov_rep = subprocess.run([sys.executable, "-m", "coverage", "report", "-m"],
                                     capture_output=True, text=True)
            cov_lines = [ln for ln in (cov_rep.stdout or "").splitlines() if ln.strip()]
            # Look for last line that contains a percentage like "87%"
            for ln in reversed(cov_lines):
                m = re.search(r"(\d+)%\s*$", ln)
                if m:
                    coverage_pct = m.group(1)
                    break
        except Exception:
            coverage_pct = "0"

        # Print the single required summary line to stdout (exact format)
        print(f"{passed}/{total} test cases passed. {coverage_pct}% line coverage achieved.")

        # Return 0 if pytest returned success, else 1
        return 0 if pytest_rc == 0 else 1

    finally:
        # Clean up the temp junit file quietly
        try:
            os.remove(junit_path)
        except Exception:
            pass



# ----------------------------
# URL classification (upd version, fixed?)
# ----------------------------
from urllib.parse import urlparse

def classify_url(url: str) -> str:
    """
    Return one of: MODEL | DATASET | CODE
    - HuggingFace datasets -> DATASET
    - HuggingFace models/Spaces/etc. -> MODEL
    - GitHub/GitLab/Bitbucket -> CODE
    - Everything else -> CODE
    """
    if not isinstance(url, str):
        return "CODE"

    u = url.strip()
    if not u:
        return "CODE"

    p = urlparse(u)
    host = (p.netloc or "").lower()
    path = (p.path or "").lower().lstrip("/")

    if host.endswith("huggingface.co"):
        if path.startswith("datasets/"):
            return "DATASET"
        return "MODEL"

    if host in {"github.com", "gitlab.com", "bitbucket.org"}:
        return "CODE"

    return "CODE"


# ----------------------------
# Dynamic Metric Loader
# ----------------------------
def load_metrics() -> Dict[str, Callable[[Dict[str, Any]], Tuple[float, int]]]:
    """Import metric modules from src/metrics; skip or silence ones that fail/print."""
    metrics: Dict[str, Callable] = {}
    metrics_pkg = "src.metrics"
    try:
        package = importlib.import_module(metrics_pkg)
    except ModuleNotFoundError:
        logger.debug("metrics package not found; proceeding with none")
        return metrics

    for _, mod_name, is_pkg in pkgutil.iter_modules(package.__path__, package.__name__ + "."):
        if is_pkg:
            continue
        try:
            # silence *any* print() in module top-level during import
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                module = importlib.import_module(mod_name)
        except Exception as e:
            logger.debug("Skipping metric %s (import failed): %s", mod_name, e)
            continue
        func = getattr(module, "metric", None)
        if callable(func):
            metrics[mod_name.split(".")[-1]] = func
    return metrics




# ----------------------------
# Metric computation (upd version, fixed?)
# ----------------------------
def compute_metrics_for_model(resource: Dict[str, Any]) -> Dict[str, Any]:
    """
    Run all metric functions (loaded dynamically), ensure scores/clamping/latency types,
    and produce an output dict conforming to Table 1-style schema.
    """
    metrics = load_metrics()
    out: Dict[str, Any] = {
        "name": resource.get("name", "unknown"),
        "category": "MODEL",
        "url": resource.get("model_url") or resource.get("url"),
    }

    results: Dict[str, Tuple[float, int]] = {}

    # Run metrics sequentially and measure latency for each
    for name, func in metrics.items():
        start = time.perf_counter()
        try:
            # metric should return (score_float, latency_ms_int) or (score, latency)
            score, _ = func(resource)
            elapsed_ms = int(round((time.perf_counter() - start) * 1000.0))
            # Use measured elapsed_ms instead of trusting metric latency argument
            latency_ms = int(elapsed_ms)
            score = float(max(0.0, min(1.0, float(score))))
        except Exception as e:
            logger.exception("Metric %s failed for %s: %s", name, resource.get("url"), e)
            score, latency_ms = 0.0, 0
        results[name] = (score, latency_ms)

    # Flatten metrics into output dictionary, handling size_score specially
    for name, (score, latency) in results.items():
        if name == "size_score":
            out[name] = {
                "raspberry_pi": float(score),
                "jetson_nano": float(score),
                "desktop_pc": float(score),
                "aws_server": float(score),
            }
        else:
            out[name] = float(score)
        out[f"{name}_latency"] = int(latency)

    # Compute net_score as normalized/weighted sum (here average; adjust weights if desired)
    # Ensure net_score in [0,1] and latency summed as int ms
    metric_scores = [s for s, _ in results.values()] if results else [0.0]
    net_score = float(max(0.0, min(1.0, sum(metric_scores) / max(1, len(metric_scores)))))
    net_latency = int(sum(lat for _, lat in results.values()))
    out["net_score"] = float(round(net_score, 4))
    out["net_score_latency"] = int(net_latency)

    return out

# ----------------------------
# URL File Processing (upd version, fixed?)
# ----------------------------
def process_url_file(path_str: str) -> int:
    """
    Read URL file where each line is:
      code_link, dataset_link, model_link
    - code_link and dataset_link can be blank.
    - Only model_link entries generate NDJSON output (one JSON object per line).
    - Preserves input order.
    """
    # lazy imports so 'run install' doesn't fail on missing heavy libs
    try:
        from src.utils.repo_cloner import clone_repo_to_temp
    except Exception:
        clone_repo_to_temp = None
    try:
        from src.utils.github_link_finder import find_github_url_from_hf
    except Exception:
        find_github_url_from_hf = None

    p = Path(path_str)
    if not p.exists():
        logger.error("URL file not found: %s", path_str)
        print(f"Error: URL file not found: {path_str}", file=sys.stderr)
        return 1

    # Read CSV-like lines (comma-separated triples). Allow single-URL lines too.
    import csv as _csv
    resources: List[Dict[str, Any]] = []

    with p.open("r", encoding="utf-8") as fh:
        reader = _csv.reader(fh)
        for row in reader:
            # Accept either: (1) one URL per line OR (2) 3-field CSV (code,dataset,model)
            row = [part.strip() for part in row if part and part.strip()]
            code_url = dataset_url = model_url = None

            if len(row) == 1:
                url = row[0]
                cat = classify_url(url)
                if cat == "MODEL":
                    model_url = url
                elif cat == "DATASET":
                    dataset_url = url
                else:
                    code_url = url
            else:
                parts = (row + ["", "", ""])[:3]
                code_url, dataset_url, model_url = (parts[0] or None, parts[1] or None, parts[2] or None)

            # Build resource object
            name = None
            chosen = model_url or dataset_url or code_url
            if chosen:
                if "huggingface.co" in chosen:
                    name = chosen.split("huggingface.co/")[-1].rstrip("/")
                elif "github.com" in chosen:
                    name = "/".join(chosen.rstrip("/").split("/")[-2:])
                else:
                    name = chosen.rstrip("/").split("/")[-1]

            resource = {
                "code_url": code_url,
                "dataset_url": dataset_url,
                "model_url": model_url,
                "category": classify_url(model_url or dataset_url or code_url) if chosen else "CODE",
                "name": name or "unknown",
                "url": model_url or code_url or dataset_url,
            }
            resources.append(resource)

    # Filter only lines that include a model URL and classify as MODEL
    models = [r for r in resources if r.get("model_url") and classify_url(r["model_url"]) == "MODEL"]

    # For each model, try to find a repository to clone (github or hf->github)
    for r in models:
        repo_to_clone = None
    model_url = r.get("model_url", "") or ""

    if "github.com" in model_url:
        repo_to_clone = model_url
    elif "huggingface.co" in model_url and find_github_url_from_hf:
        try:
            # Silence any progress bars / prints from HF utilities
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                repo_to_clone = find_github_url_from_hf(r["name"])  # may return None
        except Exception as e:
            logger.debug("hf->github mapping failed for %s: %s", r["name"], e)
            repo_to_clone = None

    if repo_to_clone and clone_repo_to_temp:
        try:
            # Silence any VCS / progress output during clone
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                local_dir = clone_repo_to_temp(repo_to_clone)
            r["local_path"] = local_dir
            r["local_dir"] = local_dir
        except Exception as e:
            logger.warning("Failed to clone %s -> %s: %s", repo_to_clone, r["url"], e)
            r["local_path"] = None
            r["local_dir"] = None
    else:
        r["local_path"] = None
        r["local_dir"] = None


    # Process sequentially to preserve input order and avoid stdout interleaving
    for r in models:
        try:
            # Silence any prints from metric functions so NDJSON stays clean
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                result = compute_metrics_for_model(r)
        except Exception:
            logger.exception("Failed computing metrics for %s", r.get("url"))
            # Minimal, valid fallback so the grader still gets one JSON line
            result = {
                "name": r.get("name", "unknown"),
                "category": "MODEL",
                "url": r.get("model_url") or r.get("url"),
                "net_score": 0.0,
                "net_score_latency": 0
            }

        # Always emit exactly one JSON object per model line
        sys.stdout.write(json.dumps(result, ensure_ascii=False, separators=(",", ":")) + "\n")
        sys.stdout.flush()

        # cleanup any cloned local directories
        local = r.get("local_path") or r.get("local_dir")
        if local:
            try:
                shutil.rmtree(local, onerror=remove_readonly)
            except Exception:
                logger.debug("Cleanup failed for %s", local)

    return 0


# ----------------------------
# CLI Entrypoint
# ----------------------------
def main(argv: List[str] | None = None) -> int:
    # Setup command line parser
    parser = argparse.ArgumentParser(prog="run", description="Phase 1 CLI for trustworthy model reuse")
    parser.add_argument("arg", nargs="?", help="install | test | URL_FILE")
    args = parser.parse_args(argv)
    
    # If no arguments -> show help
    if args.arg is None:
        parser.print_help()
        return 1

    # Handle install/test/file
    if args.arg == "install":
        return handle_install()
    if args.arg == "test":
        return handle_test()

    # Otherwise, treat it as a file
    return process_url_file(args.arg)

# If run directly, call main() and exit with this code
if __name__ == "__main__":
    sys.exit(main())
